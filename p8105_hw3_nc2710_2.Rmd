---
title: "p8105_hw3_nc2710"
author: "Nicole Comfort"
date: "10/5/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

devtools::install_github("thomasp85/patchwork")

library(tidyverse)
library(ggplot2)
library(ggridges)
library(readxl)
library(dplyr)
library(janitor)
library(patchwork)
library(viridis)
# install.packages("kableExtra")
# library(kableExtra)

# set options for figures 
# knitr::opts_chunk$set(
#   fig.width = 6,
#   fig.asp = .6,
#   out.width = "95%"
# )

theme_set(theme_bw() + theme(legend.position = "bottom"))

```

## Problem 1 

This problem uses data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010, accessed from data.gov in Sept 2018 and downloaded from the p8105 github.

The dataset initially contains 134,203 rows and 23 columns. There is information on location, topic, question, response, and response number. The data is structured so that each (multiple-choice) response to each question is a separate row. A complete data dictionary is available online.

```{r download brfss dataset, cache = TRUE}

# install.packages("devtools")
library(devtools)
install_github("p8105/p8105.datasets", force = TRUE)

library("p8105.datasets")
data("brfss_smart2010") # load the data

```

Do some data cleaning: 

* Format the data to use appropriate variable names

* Focus on the "Overall Health" topic

* Include only responses from "Excellent" to "Poor"

* Organize responses as a factor taking levels ordered from "Excellent" to "Poor" 

```{r clean brfss dataset}

library(janitor)

brfss_smart2010 =
  clean_names(brfss_smart2010) %>% # format the data to use appropriate variable names
  filter(topic == "Overall Health") %>% # focus on the “Overall Health” topic, responses "Excellent" to "Poor"
  mutate(response = as.factor(response)) %>% # organize responses as a factor with 5 levels
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) # order

```

*Using this dataset, do or answer the following (commenting on the results of each)*:

* In 2002, which states were observed at 7 locations? 

The following states were observed at 7 locations in 2002: CT, FL, and NC. 

```{r}

brfss_2002 = 
  filter(brfss_smart2010, year == "2002") %>% # focus on year 2002
  distinct(locationdesc, .keep_all = TRUE) %>% # view the distinct locations for each state
  group_by(locationabbr) %>%  # group by state
  summarize(n = n()) # count the number of unique locations per state

```

* Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010.

The spaghetti plot is useful for showing general trends over time, such as the increase in locations surveyed in later years (2008-2010). However, it's difficult to identify the number of distinct locations surveyed for any individual state, because the lines overlap and the colors are too similar. Graphing by area of the country may be more informative. 

```{r  spaghetti plot}

# organize data
brfss_spaghetti =
  distinct(brfss_smart2010, locationdesc, .keep_all = TRUE) %>% 
  group_by(locationabbr, year) %>% 
  rename(State = locationabbr) %>% 
  summarize(n = n())

# make spaghetti plot
ggplot(brfss_spaghetti, aes(x = year, y = n)) + 
  geom_line(aes(color = State, alpha = 0.5)) + 
  labs(
    title = "Number of Locations Surveyed per State from 2002-2010",
    x = "Year",
    y = "Number of Distinct Locations (Counties)",
    caption = "Data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART), 2002-2010"
  ) +
  theme_bw()

```

* Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

The table shows that the average proportion of "Excellent" responses was highest in the year 2002 (24.0%), although this year also had the largest standard deviation (4.486). The standard deviation is decreasing over time, suggesting that perhaps more people/locations are being surveyed over time. The proportion of "Excellent" responses doesn't seem to vary too much over time, with a value of 24.0% in 2002, 22.5% in 2006, and 22.7% in 2010. 

```{r table}

# create new dataset which focuses on NY state with years of interest and creates a new variable for the proportion of "excellent" response values 

brfss_table_data =
  filter(brfss_smart2010,
         locationabbr == "NY",
         year == "2002" | year == "2006" | year == "2010",
         response == "Excellent") %>%
  rename(Year = year) %>% 
  group_by(Year) %>% 
  summarize(mean_excellent = mean(data_value),   
            sd_excellent = sd(data_value)) %>% 
  knitr::kable(digits = 1,
               caption = "Proportion of 'Excellent' Responses Across Locations in NY State",
               col.names = c("Year", "Mean", "Standard Deviation"))

```

* For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

The five-panel plot shows the average proportion in each response category in the "Overall Health" topic, from "Excellent" to "Poor", averaged across county-level regions in each state. A blue loess smoothed conditional mean curve and 95% confidence interval is displayed. From this, we can see that "Very good" makes up the highest proportion of responses, followed by "good", "excellent", "fair", and "poor". It appears as though this proportion for each Overall Health state is not changing much over time. 

```{r five-panel plot}

# For each year and state, compute the average proportion in each response category (taking the average across locations in a state). 
brfss_panel = 
  group_by(brfss_smart2010, year, locationabbr, response) %>% # group by year, state, response category
  mutate(avg_response = mean(data_value)) 

# Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time. 

dist_exc_p = brfss_panel %>% 
  filter(response == "Excellent") %>% 
  ggplot(aes(x = year, y = avg_response)) +
  geom_line(aes(color = locationabbr, alpha  = 0.5)) + 
  geom_smooth(method = "loess", se = TRUE) +
  labs(title = "Excellent") +
  ylim(0, 50) +
  theme(legend.position = "none")

dist_vry_gd_p = brfss_panel %>% 
  filter(response == "Very good") %>% 
  ggplot(aes(x = year, y = avg_response)) +
  geom_line(aes(color = locationabbr, alpha  = 0.5)) + 
  geom_smooth(method = "loess", se = TRUE) +
  labs(title = "Very Good") +
  ylim(0, 50) +
  theme(legend.position = "none")

dist_good_p = brfss_panel %>% 
  filter(response == "Good") %>% 
  ggplot(aes(x = year, y = avg_response)) +
  geom_line(aes(color = locationabbr, alpha  = 0.5)) + 
  geom_smooth(method = "loess", se = TRUE) +
  labs(title = "Good") +
  ylim(0, 50) +
  theme(legend.position = "none")

dist_fair_p = brfss_panel %>% 
  filter(response == "Fair") %>% 
  ggplot(aes(x = year, y = avg_response)) +
  geom_line(aes(color = locationabbr, alpha  = 0.5)) + 
  geom_smooth(method = "loess", se = TRUE) +
  labs(title = "Fair") +
  ylim(0, 50) +
  theme(legend.position = "none")

dist_poor_p = brfss_panel %>% 
  filter(response == "Poor") %>% 
  ggplot(aes(x = year, y = avg_response)) +
  geom_line(aes(color = locationabbr, alpha  = 0.5)) + 
  geom_smooth(method = "loess", se = TRUE) +
  labs(title = "Poor") +
  ylim(0, 50) +
  theme(legend.position = "right")

# layer plots together
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "95%"
)

dist_exc_p + dist_vry_gd_p + dist_good_p + dist_fair_p + dist_poor_p + plot_layout(ncol = 2, nrow = 3)
  

```

## Problem 2

This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package (it’s called instacart).

```{r load instacart data, cache = TRUE}

# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets", force = TRUE)

library("p8105.datasets")
data("instacart")

```

*The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations.*

The Instacart dataset contains `r nrow(instacart)` rows by `r ncol(instacart)` columns. The dataset contains information related to certain grocery products, such as a product's ID and location (aisle and department). For example, from the dataset we can see that Bulgarian Yogurt (product ID = 49302) is a product in the yogurt aisle (aisle ID = 120), which is in the dairy and eggs department. This information in the dataset comes from a dataset made of over 3 million online grocery orders from more than 200,000 Instacart users. 

The dataset also contains information related to the ordering of a certain product, such as whether it's been reordered and if so, the number of days since the previous order as well as the day of week and hour of the day the order is placed. The day of the week is numbered from 0-6, assuming that 0 corresponds to Sunday sequentially until 6, corresponding to Saturday. The hour of the day ranges from 0 (12am) to 23 (11pm). The 'add_to_cart_order' variable tells the order in which a given item was added to the shopping cart. The dataset is composed of integer and character variables (all variables are integers except for 'eval_set', 'product_name', 'aisle', and 'department'.) The 'reordered' variable is binary.

*Do or answer the following (commenting on the results of each)*:

* How many aisles are there, and which aisles are the most items ordered from?

There are `r nrow(distinct(instacart, aisle_id))` aisles.

The most items are ordered from the Fresh Vegetables (150,609), Fresh Fruits (150,773), and Package Vegetables and Fruits (78,493). Perhaps items are ordered most frequently from these aisles because the food tends to spoil the fastest. 

Banana is the most ordered item, followed by bag of organic bananas, followed by organic strawberries. All of these items are from the 'fresh fruits' aisle. In fact, the top 10 most ordered items all come from either the 'fresh fruits' aisle or 'packaged vegetables fruits' aisle.

```{r}

# how many aisles are there
nrow(distinct(instacart, aisle_id))

# which items are most ordered 
most_ordered_items =
  instacart %>% 
  group_by(product_name, aisle) %>% 
  summarize(n = n())

# which aisles are the most items ordered from
instacart %>% 
  group_by(aisle)  %>% 
  summarize(n = n())  %>% 
  arrange(desc(n))

```

* Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

This plot shows the number of items ordered in each aisle. Note that the x axis is ordered by aisle ID number. The Fresh Vegetables (150,609), Fresh Fruits (150,773), and Package Vegetables and Fruits (78,493) receive more than 3x the number of items ordered in comparison with other aisles, which tend to have about 50,000 or fewer orders. 

```{r}

items_ordered = instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_orders = n())

as.tibble(items_ordered) %>% 
  ggplot(aes(x = aisle_id, y = number_orders)) +
  geom_bar(stat = "identity") +
    labs(
    title = "Number of Items Ordered in Each Aisle",
    x = "Aisle ID",
    y = "Number of Items Ordered",
    caption = "Data from the Instacart data, accessed Oct 2018"
  ) +
  scale_x_continuous(breaks = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140),
                     minor_breaks = waiver(),
                     labels = c("10", "20", "30", "40", "50", "60", "70", "80", "90", "100", "110", "120", "130", "140"))

```

* Make a table showing the most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

From the table, we can see that the most popular baking ingredient is the 100% Organic Unbleached All-Purpose Flour. The most popular dog food item is the Chopped Blends With Lamb Brown Rice Carrots Tomatoes & Spinach Dog Food, while the most popular packaged vegetables fruits item is Super Greens Salad. 

```{r table of popular items}

pop_items_table =
  instacart %>%
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle) %>%
  summarize(most_pop_item = first(product_name)) %>%
  knitr::kable(col.names = c("Aisle", "Most Popular Item"))
  
print(pop_items_table)

```

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

The hour of the day is based on a 24-hr clock, with the decimals representing the fraction of an hour. We assume that "0" in the original dataset corresponds to Sunday. From the table, we can ascertain that ice cream is ordered later in the day than is Pink Lady Apples, except for Sunday, when perhaps people are doing all their weekly grocery shopping at once. 

```{r mean hour of day table}

hr_of_day_table =
  instacart %>%
  select(product_name, order_dow, order_hour_of_day) %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_hr = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hr) %>% 
  knitr::kable(., col.names = c("Product Name",
                               "Sunday",
                               "Monday",
                               "Tuesday",
                               "Wednesday",
                               "Thursday",
                               "Friday",
                               "Saturday"))

print(hr_of_day_table)

```

## Problem 3 

This problem uses the NY NOAA data, downloaded from the p8105.datasets package called ny_noaa. 

```{r import ny_noaa dataset}

# install.packages("devtools")
library(devtools)
install_github("p8105/p8105.datasets", force = TRUE)

library("p8105.datasets")
data("ny_noaa")

# list rows of data that have missing values 
missing_noaa = ny_noaa[!complete.cases(ny_noaa),]

```

*The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.* 

The ny_noaa dataset is a large dataset which comes from the New York state weather stations from Jan 1, 1981 to Dec 31, 2010; the dataset is comprised of `r nrow(ny_noaa)` observations of `r nrow(ny_noaa)` variables. These key variables are ID (character, for weather station ID), Date (date), Precipitation (integer), Snow (integer), snow depth (integer), and maximum/minimum observed temperature, which is stored as character variables. I'm also assuming (based on looking at the values) that the temperature variable is in the units of Fahrenheit. 

Missing data is definitely an issue in this dataset. When I create a tibble comprised of missing values, it results in a dataset comprised of `r nrow(missing_noaa)` observations of `r nrow(missing_noaa)` variables. This means that more than half of the ny_noaa data (almost 53%) contains some missing data. 

*Then, do or answer the following (commenting on the results of each)*:

* Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

In my data cleaning, I created variables for year, month, and day and converted the units for the precipitation variable to millimeters and the units for max and min temp to degrees Celsius. 

I noticed that the minimum for the "snow" variable is one observation reading -13, which is not reasonable/expected and therefore I'm assuming would be an input error. I would go back to the original data collection form to double-check the recorded value. Otherwise, the minimum is 0 as expected and appears in many cases. 

For snowfall, the most commonly observed values are 0mm and NA. This makes sense because there is a lot of missing data and also, most days out of the year it is not snowing in NY. 

```{r ny_noaa data cleaning}

library(lubridate)

ny_noaa = mutate(ny_noaa, year = year(ny_noaa$date)) # new variable for year
ny_noaa = mutate(ny_noaa, month = month(ny_noaa$date)) # new variable for month
ny_noaa = mutate(ny_noaa, day = day(ny_noaa$date)) # new variable for day 

ny_noaa %>%
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         prcp = prcp / 10,
         tmax = tmax / 10,
         tmin = tmin / 10)

# are observations for temp, precipitation, and snowfall in reasonable units? 
max(ny_noaa$prcp, na.rm = TRUE) 
min(ny_noaa$prcp, na.rm = TRUE)

max(ny_noaa$tmax, na.rm = TRUE) # assuming temp is in Fahrenheit
min(ny_noaa$tmax, na.rm = TRUE) 

max(ny_noaa$tmin, na.rm = TRUE)
min(ny_noaa$tmin, na.rm = TRUE)

max(ny_noaa$snow, na.rm = TRUE) # what value is this?? 
min(ny_noaa$snow, na.rm = TRUE) # - 13 for snow doesn't make sense...

# For snowfall, what are the most commonly observed values? Why? 
ny_noaa_snow = ny_noaa %>% 
  group_by(snow) %>%  # group by snow
  summarize(n = n())

```

* Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

```{r Jan, July avg temp}

# Data manipulation
ny_noaa = 
  mutate(ny_noaa, tmax = as.numeric(tmax), tmin = as.numeric(tmin)) %>% # coerce to correct format
  mutate(avg_temp = ((ny_noaa$tmax + ny_noaa$tmin)/2)) # new var for avg temp 

ny_noaa_jan = 
  filter(ny_noaa, month == "1") %>% # look at Jan
  group_by(id, year) %>% # look at each station and year
  mutate(avg_tmp_jan = mean(avg_temp)) 
  
ny_noaa_jul =
  filter(ny_noaa, month == "7") %>% # look at Jul
  group_by(id, year) %>% # look at each station and year
  mutate(avg_tmp_jul = mean(avg_temp)) 

# Make two plots
avg_tmp_jan_plot =
  ggplot(ny_noaa_jan, aes(x = year, y = avg_tmp_jan)) + 
  geom_line(aes(color = id)) + 
  labs(
    title = "Average Temperature Recorded at Weather Stations Across Years",
    x = "Year",
    y = "Weather Station ID",
    caption = "Data from the NY NOAA, acccessed Oct 2018"
  ) +
  theme_bw()
  
avg_tmp_jul_plot = 
  ggplot(ny_noaa_jul, aes(x = year, y = avg_tmp_jul)) + 
  geom_line(aes(color = id)) + 
  labs(
    title = "Average Temperature Recorded at Weather Stations Across Years",
    x = "Year",
    y = "Weather Station ID",
    caption = "Data from the NY NOAA, acccessed Oct 2018"
  ) +
  theme_bw()

# # layer plots together
# knitr::opts_chunk$set(
#   fig.width = 6,
#   fig.asp = .6,
#   out.width = "95%"
# )

avg_tmp_jan_plot + avg_tmp_jul_plot + plot_layout(ncol = 1)

```

* Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r tmax vs. tmin}

library(hexbin)

# plot tmax vs tmin for full dataset
ny_noaa %>% 
  filter(!is.na(tmax)) %>% # omit rows without precipitation data
  filter(!is.na(tmin)) %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  labs(
    title = "Maximum vs. Minimum Temperature for NY NOAA Weather Data",
    x = "Minimum Temperature (units)",
    y = "Maximum Temperature (units)"
  ) +
  theme(legend.position = 'bottom', 
        legend.text = element_text(margin = margin(t = 10)))

# plot distribution of snowfall values greater than 0 and less than 100
ny_noaa_snow = 
  filter(ny_noaa, !is.na(snow)) %>% # omit rows without snow data
  filter(snow > 0 & snow < 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_violin(aes(fill = year)) 

# layer plots together 
  
```

